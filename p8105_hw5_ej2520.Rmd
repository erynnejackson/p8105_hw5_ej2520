---
title: "p8105_hw5_ej2520"
author: "Erynne Jackson"
date: "2024-11-15"
output: html_document
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
library(tidyverse)


knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE, 
  fig.width = 6, 
  fig.asp = .6, 
  out.width = "90%"
) 

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options( 
  ggplot2.continuous.colour = "viridus", 
  ggplot2continous.fill = "viridus"
  )

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
                      
library(rvest)

set.seed(1)

```

# Problem 1

```{r}

birth_sim = function(n){
birthday = sample(1:365, size = n, replace = TRUE)

duplicate = length(unique(birthday)) < n

return(duplicate)

}

birth_sim(50)
```

```{r}
sim_res = 
  expand_grid(
    n = 2:50, 
    iter = 1:1000
  ) |> 
  mutate(res = map_lgl(n, birth_sim)) |>
group_by(n) |> 
summarize(prob = mean(res))


sim_res |> 
  ggplot(aes(x = n, y=prob)) + 
  geom_line()
```


# Problem 2

```{r}

Power_data = tibble(
  x = rnorm(30,0,5)
) 

Power_data |> 
  summarize( 
    mean_x= mean(x), 
    sd_x= sd(x)
  )


power = function(n) {
  sim_data = rnorm(n, 0, 5)
  return(tibble(mean_x = mean(sim_data), sd_x = sd(sim_data)))
}


output = vector("list", 5000)
for (i in 1:5000) { 
  output[[i]] = power(30)
}

sim_res = bind_rows(output)
```


```{r}

n = 30              
sigma = 5           
alpha = 0.05        
num_sim = 5000
mu_values = c(0,1,2,3,4,5,6)

output = vector("list",.100) 

for(mu in mu_values) {
  estimates = numeric(num_sim)
  p_values = numeric(num_sim)
  

  for(i in 1:5000) {
    x <- rnorm(n, mean = mu, sd = sigma)
    
    test_res = t.test(x, mu = 0)
    estimates[i] = test_res$estimate
    p_values[i] = test_res$p.value
  }
  

  power = mean(p_values < alpha)
  avg_est = mean(estimates)
  avg_est_rejected = mean(estimates[p_values < alpha])


output[[as.character(mu)]] = list(
    power = power,
    avg_est= avg_est,
    avg_est_rejected = avg_est_rejected
  )
}

final_test = data.frame(
  mu = rep(mu_values, each = 1),
  power = sapply(output, function(x) x$power),
  avg_est= sapply(output, function(x) x$avg_est),
  avg_est_rejected = sapply(output, function(x) x$avg_est_rejected)
)


power.true = ggplot(final_test, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(title = "Power of the Test",
       x = "True Value of μ",
       y = "Proportion Rejected (Power)")


average.true = ggplot(final_test, aes(x = mu, y = avg_est)) +
  geom_line() +
  geom_point() +
  labs(title = "Average Estimate of μ",
       x = "True Value of μ",
       y = "Average Estimate of μ") +
  geom_line(aes(x = mu, y = avg_est_rejected), color = "pink") +
  geom_point(aes(x = mu, y = avg_est_rejected), color = "pink") 

# Print the results
print(average.true)
print(power.true)



```

As the sample size increases, power also increases. The sample average of the estimated mu across tests for which the null is rejected is not approximately equal to the true value of mu until the true mu and estimated mu are equal to 4. This may be because the sample mean depends on the sample size. The larger the sample size, the closer we get to the true mean. 

# Problem 3 


The raw homicide dataset contains 52,179 observations and 12 columns/variables that gives us information on homicides in 50 large U.S cities . The variables are `uid`, `reported_data`, `victim_last`, `victim_race`, `victim_age`, `victim_sex`, `city`, `state`, `lat`, `lon`, and '`disposition`. Below, I have added a new variable that combined city and state.  
```{r}

homicide = read_csv("homicide-data.csv") 


new_homicide = homicide|> 
  mutate(city_state = paste(city, state, sep = ",")) |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")), 
    .groups = "drop"
  )

```

Next, I obtained the proportion on homicides that were unsolved and pulled the estimate and confidence interval. 
```{r Estimate and CI for Baltimore}

baltimore_data = homicide |> 
  mutate(city_state = paste(city, state, sep = ",")) |>
  filter(city_state == "Baltimore,MD") |> 
  summarize(
    total_homicides_balt = n(),
    unsolved_homicides_balt = sum(disposition %in% c("Closed without arrest", "Open/No arrest"), na.rm = TRUE))

baltimore_prop = 
  prop.test(baltimore_data$unsolved_homicides_balt, baltimore_data$total_homicides_balt) |> 
  broom::tidy()

print(baltimore_prop)

```

Below, I pulled he estimate and confidence interval for every city in our homicide dataset. 
```{r CI for Every City}
new_homicide = homicide|> 
  mutate(city_state = paste(city, state, sep = ",")) |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")), 
    .groups = "drop"
  )


prop_every_city = new_homicide |> 
  mutate(
    prop_result = pmap(
      list(unsolved_homicides, total_homicides), 
      ~ prop.test(..1, ..2))) |> 
   mutate(
    prop_tidy = map(prop_result, broom::tidy)) |> 
  unnest(prop_tidy) |> 
  select(city_state, estimate, p.value, conf.low, conf.high)

print(prop_every_city)

```

Finally, I created a plot that graphs the estimate for each city with error bars. 
```{r Creating Plot}

prop_plot = prop_every_city |>
  mutate (city_state = fct_reorder(city_state, estimate)) |> 
  ggplot(aes(x = city_state, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_point(color = "black", size = 2) + 
  geom_errorbar(width = 0.1, color = "red") +  
  labs(
    x = "City/State",
    y = "Proportion of Unsolved Homicides (Estimate)",
    title = "Proportion of Unsolved Homicides by City"
  ) +
  coord_flip() +  
  theme_minimal()

print(prop_plot)
```